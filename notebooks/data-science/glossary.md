# Glossary

## A

`ACID-compliance`
:   Ensuring data accuracy and consistency through Atomicity, Consistency, Isolation, and Durability (ACID) in database transactions.

`Adobe Spark`
:   A suite of software tools that allow users to create and share visual content such as graphics, web pages, and videos.

`Algorithms`
:   A set of step-by-step instructions to solve a problem or complete a task.

`Analytical skills`
:   The ability to analyze information systematically, logically, and organized.
	
`Analytics`
:   The process of examining data to draw conclusions and make informed decisions is a fundamental aspect of data science, involving statistical analysis and data-driven insights.

`Arithmetic Models`
:   Data science often uses Mathematical models to analyze data and predict outcomes.

`Artificial Neural Networks`
:   Collections of small computing units (neurons) that process data and learn to make decisions over time.

## B

`Bayesian Analysis`
:   A statistical technique that uses Bayes' theorem to update probabilities based on new evidence.
	
`Big data`
:   Vast amounts of structured, semi-structured, and unstructured data are characterized by its volume, velocity, variety, and value, which, when analyzed, can provide competitive advantages and drive digital transformations.

`Big Data Cluster`
:   A distributed computing environment comprising thousands or tens of thousands of interconnected computers that collectively store and process large datasets.

`Broad Network Access`
:   The ability to access cloud resources via standard mechanisms and platforms such as mobile devices, laptops, and workstations over networks.

`Business Insights`
:   Accurate insights and reports generated by generative AI can be updated as data evolves, enhancing decision-making and uncovering hidden patterns.

## C

`Case study`
:   In-depth analysis of an instance of a chosen subject to draw insights that inform theory, practice, or decision-making.
	
`Chief Data Officer (CDO)`
:   An emerging role responsible for overseeing data-related initiatives, governance, and strategies, ensuring that data plays a central role in digital transformation efforts.
	
`Chief Information Officer (CIO)`
:   An executive is responsible for managing an organization's information technology and computer systems, contributing to technology-related aspects of digital transformation.
	
`Cloud Computing`
:   The delivery of on-demand computing resources, including networks, servers, storage, applications, services, and data centers, over the Internet on a pay-for-use basis.	Introduction to Cloud

`Cloud Deployment Models`
:   Categories that indicate where cloud infrastructure resides, who manages it, and how cloud resources and services are made available to users, including public, private, and hybrid models.

`Cloud Service Models`
:   Models based on the layers of a computing stack, including Infrastructure as a Service (IaaS), Platform as a Service (PaaS), and Software as a Service (SaaS), represent different cloud computing offerings.
	
`Cloud-based Integration Platform as a Service (iPaaS)`
:   Cloud-hosted integration platforms that offer integration services through virtual private clouds or hybrid cloud models, providing scalability and flexibility.

`Cluster Analysis`
:   The process of grouping similar data points together based on certain features or attributes.

`Coding Automation`
:   Using generative AI to automatically generate and test software code for constructing analytical models, freeing data scientists to focus on higher-level tasks.
	
`Column-based Database`
:   A type of NoSQL database that organizes data in cells grouped as columns, often used for systems requiring high write request volume and storage of time-series or IoT data.

`Comma-separated values (CSV) / Tab-separated values (TSV)`
:   Commonly used format for storing tabular data as plain text where either the comma or the tab separates each value. Used to store structured data.

`Commodity Hardware`
:   Standard, off-the-shelf hardware components are used in a big data cluster, offering cost-effective solutions for storage and processing without relying on specialized hardware.

`Computational thinking`
:   Breaking problems into smaller parts and using algorithms, logic, and abstraction to develop solutions. Often used but not limited to computer science.

## D

`Data Algorithms`
:   Computational procedures and mathematical models used to process and analyze data made accessible in the cloud for data scientists to deploy on large datasets efficiently.
	
`Data at rest`
:   Data that is stored and not actively in motion, typically residing in a database or storage system for various purposes, including backup.

`Data clusters`
:   A group of similar, related data points distinct from other clusters.

`Data file types`
:   A computer file configuration is designed to store data in a specific way.

`Data format`
:   How data is encoded so it can be stored within a data file type.

`Data integration`
:   A discipline involving practices, architectural techniques, and tools that enable organizations to ingest, transform, combine, and provision data across various data types, used for purposes such as data consistency, master data management, data sharing, and data migration.

`Data Lake`
:   A data repository for storing large volumes of structured, semi-structured, and unstructured data in its native format, facilitating agile data exploration and analysis.

`Data Mart`
:   A subset of a data warehouse designed for specific business functions or user communities, providing isolated security and performance for focused analytics.

`Data Mining`
:   The process of automatically searching and analyzing data to discover patterns and insights that were previously unknown. Extracting information from raw data, such as making decisions, predicting trends, or understanding phenomena.

`Data pipeline`
:   A comprehensive data movement process that covers the entire journey of data from source systems to destination systems, which includes data integration as a key component.

`Data Replication`
:   A strategy in which data is duplicated across multiple nodes in a cluster to ensure data durability and availability, reducing the risk of data loss due to hardware failures.

`Data repository`
:   A general term referring to data that has been collected, organized, and isolated for business operations or data analysis. It can include databases, data warehouses, and big data stores.

`Data Science`
:   The field involves collecting, analyzing, and interpreting data to extract valuable insights and make informed decisions. An interdisciplinary field that involves extracting insights and knowledge from data using various techniques, including programming, statistics, and analytical tools.
	
`Data strategy`
:   A plan that outlines how an organization will collect, manage, and use data to achieve its goals.

`Data visualization`
:   A visual way, such as a graph, of representing data in a readily understandable way makes it easier to see trends in the data.

`Data warehouse`
:   A central repository that consolidates data from various sources through the Extract, Transform, and Load (ETL) process, making it accessible for analytics and business intelligence.

`Decision Trees`
:   A type of machine learning algorithm used for decision-making by creating a tree-like structure of decisions.

`Deep Learning`
:   A subset of machine learning that involves artificial neural networks inspired by the human brain, capable of learning and making complex decisions from data on their own.

`Deep Learning Models`
:   Includes Generative Adversarial Networks (GANs) and Variational Autoencoders (VAEs) that create new data instances by learning patterns from large datasets.

`Delimited text file`
:   Text files are used to store data where each line or row has values separated by a delimiter. A delimiter is a sequence of one or more characters specifying the boundary between values. Common delimiters include comma, tab, colon, vertical bar, and space.

`Digital Change`
:   The integration of digital technology into business processes and operations leads to improvements and innovations in how organizations operate and deliver value to customers.

`Digital Transformation`
:   A strategic and cultural organizational change driven by data science, especially Big Data, to integrate digital technology across all areas of the organization, resulting in fundamental operational and value delivery changes.

`Distributed Data`
:   The practice of dividing data into smaller chunks and distributing them across multiple computers within a cluster enables parallel processing for data analysis.

`Document-based Database`
:   A type of NoSQL database that stores each record and its associated data within a single document, allowing flexible indexing, ad hoc queries, and analytics over collections of documents.

## E

`ETL process`
:   The Extract, Transform, and Load process for data integration involves extracting data from various sources, transforming it into a usable format, and loading it into a repository.

`Executive summary`
:   Usually occurring at the beginning of a research paper, this section summarizes the important parts of the paper, including its key findings.

`Extensible Markup Language (XML)`
:   A language designed to structure, store, and enable data exchange between various technologies.

## F

`Five V's of Big Data`
:   A set of characteristics common across Big Data definitions, including Velocity, Volume, Variety, Veracity, and Value, highlighting the rapid generation, scale, diversity, quality, and value of data.

## G

`Generative AI`
:   A subset of AI that focuses on creating new data, such as images, music, text, or code, rather than just analyzing existing data.

`Graph-based Database`
:   A type of NoSQL database that uses a graphical model to represent and store data, ideal for visualizing, analyzing, and discovering connections between interconnected data points.

## H

`Hadoop`
:   An open-source framework designed to store and process large datasets across clusters of computers. A distributed storage and processing framework used for handling and analyzing large datasets, particularly well-suited for big data analytics and data science applications.

`Hadoop Distributed File System (HDFS)`
:   A storage system within the Hadoop framework that partitions and distributes files across multiple nodes, facilitating parallel data access and fault tolerance.

`High-performing computing (HPC) cluster`
:   A computing technology that uses a system of networked computers designed to solve complex and computationally intensive problems in traditional environments.

## I

`Infrastructure as a Service (IaaS)`
:   A cloud service model that provides access to computing infrastructure, including servers, storage, and networking, without the need for users to manage or operate them.

## J

`JavaScript Object Notation (JSON)`
:   A data format compatible with various programming languages for two applications to exchange structured data.

`Java-Based Framework`
:   Hadoop is implemented in Java, an open-source, high-level programming language, providing the foundation for building distributed storage and processing solutions.

`Jupyter notebooks`
:   A computational environment that allows users to create and share documents containing code, equations, visualizations, and explanatory text.

## K

`Key-value store`
:   A type of NoSQL database where data is stored as key-value pairs, with the key serving as a unique identifier and the value containing data, which can be simple or complex.

## M

`Map Process`
:   The initial step in Hadoop’s MapReduce programming model, where data is processed in parallel on individual cluster nodes, often used for data transformation tasks.

`Market Basket Analysis`
:   Analyzing which goods tend to be bought together, often used for marketing insights.

`Mathematical computing`
:   The use of computers to calculate, simulate, and model mathematical problems.

`Matrices`
:   Plural for matric, matrices are a rectangular (tabular) array of numbers often used in mathematics, statistics, and computer science.
	
`Measured Service`
:   A characteristic where users are billed for cloud resources based on their actual usage, with resource utilization transparently monitored, measured, and reported.

`Model`
:   A representation of the relationships and patterns found in data to make predictions or analyze complex systems retaining essential elements needed for analysis.

## N

`Naive Bayes`
:   A simple probabilistic classification algorithm based on Bayes' theorem.

`Natural Language Processing (NLP)`
:   A field of AI that enables machines to understand, generate, and interact with human language, revolutionizing content creation and chatbots.

`Nearest neighbor`
:   A machine learning algorithm that predicts a target variable based on its similarity to other values in the dataset.

`Neural networks`
:   A computational model used in deep learning that mimics the structure and functioning of the human brain’s neural pathways. It takes an input, processes it using previous learning, and produces an output.

`NoSQL databases`
:   Databases are designed to store and manage unstructured data and provide analysis tools for examining this type of data.

## O

`Online Transaction Processing (OLTP) Systems`
:   Systems that focus on handling business transactions and storing structured data.

`On-Demand Self-Service`
:   The capability for users to access and provision cloud resources such as processing power, storage, and networking using simple interfaces without human interaction with service providers.

`Outliers`
:   When a data point or points occur significantly outside of most of the other data in a data set, potentially indicating anomalies, errors, or unique phenomena that could impact statistical analysis or modeling.

## P

`Pandas`
:   An open-source Python library that provides tools for working with structured data is often used for data manipulation and analysis.

`Portability`
:   The capability of data integration tools to be used in various environments, including single-cloud, multi-cloud, or hybrid-cloud scenarios, provides flexibility in deployment options.

`Precision vs. Recall`
:   Metrics are used to evaluate the performance of classification models.

`Predictive Analytics`
:   Using machine learning techniques to predict future outcomes or events. Using data, algorithms, models, and machine learning to make predictions.

`Pre-built connectors`
:   Cataloged connectors and adapters that simplify connecting and building integration flows with diverse data sources like databases, flat files, social media, APIs, CRM, and ERP applications.

`Python notebooks`
:   Also known as a “Jupyter” notebook, this computational environment allows users to create and share documents containing code, equations, visualizations, and explanatory text.

## Q

`Quantitative analysis`
:   A systematic approach using mathematical and statistical analysis is used to interpret numerical data.

## R

`R`
:   An open-source programming language used for statistical computing, data analysis, and data visualization.

`Rapid Elasticity`
:   The ability to quickly scale cloud resources up or down based on demand, allowing users to access more resources when needed and release them when not in use.

`Recommendation engine`
:   A computer program that analyzes user input, such as behaviors or preferences, and makes personalized recommendations based on that analysis.

`Reduce Process`
:   The second step in Hadoop's MapReduce model is where results from the mapping process are aggregated and processed further to produce the final output, typically used for analysis.

`Regression`
:   A statistical model that shows a relationship between one or more predictor variables with a response variable.

`Relational Databases (RDBMSes)`
:   Databases are designed to store structured data with well-defined schemas and support standard data analysis methods and tools. Databases that organize data into a tabular format with rows and columns, following a well-defined structure and schema.

`Replication`
:   The act of creating copies of data pieces within a big data cluster enhances fault tolerance and ensures data availability in case of hardware or node failures.

`Resource Pooling`
:   A cloud characteristic where computing resources are shared and dynamically assigned to multiple consumers, promoting economies of scale and cost-efficiency.

## S

`Scalability`
:   The ability of a data repository to grow and expand its capacity to handle increasing data volumes and workload demands over time.

`Schema`
:   The predefined structure that describes the organization and format of data within a database, indicating the types of data allowed and their relationships.

`Sensors`
:   Devices such as Global Positioning Systems (GPS) and Radio Frequency Identification (RFID) tags generate structured data.

`Skills Network Labs (SN Labs)`
:   Learning resources provided by IBM, including tools like Jupyter Notebooks and Spark clusters, are available to learners for cloud data science projects and skill development.

`Spilling to Disk`
:   A technique used in memory-constrained situations where data is temporarily written to disk storage when memory resources are exhausted, ensuring uninterrupted processing.

`Spreadsheets`
:   Software applications like Excel and Google Spreadsheets are used for organizing and analyzing structured data.

`SQL Databases`
:   Databases that use Structured Query Language (SQL) for defining, manipulating, and querying data in structured formats.

`Stata`
:   A software package used for statistical analysis.

`Statistical distributions`
:   A way of describing the likelihood of different outcomes based on a dataset. The “bell curve” is a common statistical distribution.
	
`STEM Classes`
:   Science, Technology, Engineering, and Mathematics (STEM) courses typically taught in high schools prepare students for technical careers, including data science.
	
`Streaming data`
:   Data that is continuously generated and transmitted in real-time requires specialized handling and processing to capture and analyze.

`Structured data`
:   Data is organized and formatted into a predictable schema, usually related tables with rows and columns.

`Structured Query Language (SQL)`
:   A language used for managing data in a relational database.

`Synthetic Data`
:   Artificially generated data with properties similar to real data, used by data scientists to augment their datasets and improve model training.

## T

`Tabular data`
:   Data that is organized into rows and columns.

`Tab-separated values (TSVs)`
:   Delimited text files where the delimiter is a tab. Used as an alternative to CSV when literal commas are present in text data.

`TCP/IP network`
:   A network that uses the TCP/IP protocol to communicate between connected devices on that network. The Internet uses TCP/IP.

## U

`Unstructured data`
:   Unorganized data that lacks a predefined data model or organization makes it harder to analyze using traditional methods. This data type often includes text, images, videos, and other content that doesn’t fit neatly into rows and columns like structured data.
	
`Use cases for relational databases`
:   Applications such as Online Transaction Processing (OLTP), Data Warehouses (OLAP), and IoT solutions where relational databases excel.

## V

`Vendor lock-in`
:   A situation where a user becomes dependent on a specific vendor’s technologies and solutions, making it challenging to switch to other platforms.
	
`Variety`
:   The diversity of data types, including structured and unstructured data from various sources such as text, images, video, and more, posing data management challenges.

`Velocity`
:   The speed at which data accumulates and is generated, often in real-time or near-real-time, drives the need for rapid data processing and analytics.

`Veracity`
:   The quality and accuracy of data, ensuring that it conforms to facts and is consistent, complete, and free from ambiguity, impacts data reliability and trustworthiness.

`Video Tracking System`
:   A system used to capture and analyze video data from games, enabling in-depth analysis of player movements and game dynamics, contributing to data-driven decision-making in sports.

`Volume`
:   The scale of data generated and stored is driven by increased data sources, higher-resolution sensors, and scalable infrastructure.

## X

`XLSX`
:   The Microsoft Excel spreadsheet file format.
