# Data Literacy

## Data Collection and Organization

A **data repository** is a general term used to refer to data that has been collected, organized, and isolated so that it can be used for business operations or mined for reporting and data analysis. It can be a small or large database infrastructure with one or more databases that collect, manage, and store data sets.

A **database** is a collection of data, or information, designed for the input, storage, search and retrieval, and modification of data. And a **Database Management System, or DBMS**, is a set of programs that creates and maintains the database. It allows you to store, modify, and extract information from the database using a function called querying. For example, if you want to find customers who have been inactive for six months or more, using the query function, the database management system will retrieve data of all customers from the database that have been inactive for six months and more. Even though a database and DBMS mean different things the terms are often used interchangeably.

There are different types of databases. Several factors influence the choice of database, such as the data type and structure, querying mechanisms, latency requirements, transaction speeds, and intended use of the data. Two main types of databases: relational and non-relational databases.

**Relational databases**, also referred to as RDBMSes, build on the organizational principles of flat files, with data organized into a tabular format with rows and columns following a well-defined structure and schema. However, unlike flat files, RDBMSes are optimized for data operations and querying involving many tables and much larger data volumes. Structured Query Language, or SQL, is the standard querying language for relational databases.

**Non-relational databases**, also known as NoSQL, or “Not Only SQL”. Non-relational databases emerged in response to the volume, diversity, and speed at which data is being generated today, mainly influenced by advances in cloud computing, the Internet of Things, and social media proliferation. Built for speed, flexibility, and scale, non-relational databases made it possible to store data in a schema-less or free-form fashion. NoSQL is widely used for processing big data.

A **data warehouse** works as a central repository that merges information coming from disparate sources and consolidates it through the extract, transform, and load process, also known as the **ETL process**, into one comprehensive database for analytics and business intelligence. At a very high-level, the ETL process helps you to extract data from different data sources, transform the data into a clean and usable state, and load the data into the enterprise’s data repository. Related to Data Warehouses are the concepts of Data Marts and Data Lakes, which we will cover later. Data Marts and Data Warehouses have historically been relational, since much of the traditional enterprise data has resided in RDBMSes. However, with the emergence of NoSQL technologies and new sources of data, non-relational data repositories are also now being used for Data Warehousing. Another category of data repositories are Big Data Stores, that include distributed computational and storage infrastructure to store, scale, and process very large data sets. Overall, data repositories help to isolate data and make reporting and analytics more efficient and credible while also serving as a data archive.

## Relation Database Management System

A relational database is a collection of data organized into a table structure, where the tables can be linked, or related, based on data common to each. Tables are made of rows and columns, where rows are the “records”, and the columns the “attributes”.

Let’s take the example of a customer table that maintains data about each customer in a company. The columns, or attributes, in the customer table are the Company ID, Company Name, Company Address, and Company Primary Phone; and Each row is a customer record. Now let’s understand what we mean by tables being linked, or related, based on data common to each. Along with the customer table, the company also maintains transaction tables that contain data describing multiple individual transactions pertaining to each customer. The columns for the transaction table might include the Transaction Date, Customer ID, Transaction Amount, and Payment Method. The customer table and the transaction tables can be related based on the common Customer ID field. You can query the customer table to produce reports such as a customer statement that consolidates all transactions in a given period.

This capability of relating tables based on common data enables you to retrieve an entirely new table from data in one or more tables with a single query. It also allows you to understand the relationships among all available data and gain new insights for making better decisions.

Relational databases build on the organizational principles of flat files such as spreadsheets, with data organized into rows and columns following a well-defined structure and schema. But this is where the similarity ends. Relational databases, by design, are ideal for the optimized storage, retrieval, and processing of data for large volumes of data, unlike spreadsheets that have a limited number of rows and columns. Each table in a relational database has a unique set of rows and columns and relationships can be defined between tables, which minimizes data redundancy. Moreover, you can restrict database fields to specific data types and values, which minimizes irregularities and leads to greater consistency and data integrity.

Relational databases use SQL for querying data, which gives you the advantage of processing millions of records and retrieving large amounts of data in a matter of seconds. Moreover, the security architecture of relational databases provides controlled access to data and also ensures that the standards and policies for governing data can be enforced.

Relational databases range from small desktop systems to massive cloud-based systems. They can be either: open-source and internally supported, open-source with commercial support, or commercial closed-source systems. IBM DB2, Microsoft SQL Server, MySQL, Oracle Database, and PostgreSQL are some of the popular relational databases. Cloud-based relational databases, also referred to as Database-as-a-Service, are gaining wide use as they have access to the limitless compute and storage capabilities offered by the cloud. Some of the popular cloud relational databases include Amazon Relational Database Service (RDS), Google Cloud SQL, IBM DB2 on Cloud, Oracle Cloud, and SQL Azure. RDBMS is a mature and well-documented technology, making it easy to learn and find qualified talent.

### Advantages
One of the most significant advantages of the relational database approach is its ability to create meaningful information by joining tables. Some of its other advantages include:
- Flexibility
	Using SQL, you can add new columns, add new tables, rename relations, and make other changes while the database is running and queries are happening.
- Reduced redundancy
	Relational databases minimize data redundancy. For example, the information of a customer appears in a single entry in the customer table, and the transaction table pertaining to the customer stores a link to the customer table.
- Ease of backup and disaster recovery
	Relational databases offer easy export and import options, making backup and restore easy. Exports can happen while the database is running, making restore on failure easy. Cloud-based relational databases do continuous mirroring, which means the loss of data on restore can be measured in seconds or less.
- ACID-compliance
	ACID stands for Atomicity, Consistency, Isolation, and Durability. And ACID compliance implies that the data in the database remains accurate and consistent despite failures, and database transactions are processed reliably.
	
### Use Cases
Some use cases for relational databases:
- Online Transaction Processing:
	OLTP applications are focused on transaction-oriented tasks that run at high rates. Relational databases are well suited for OLTP applications because they can accommodate a large number of users; they support the ability to insert, update, or delete small amounts of data; and they also support frequent queries and updates as well as fast response times.
- Data warehouses
	In a data warehousing environment, relational databases can be optimized for online analytical processing (or OLAP), where historical data is analyzed for business intelligence.
- IoT solutions
	Internet of Things (IoT) solutions require speed as well as the ability to collect and process data from edge devices, which need a lightweight database solution.

### Limitations
RDBMS does not work well with semi-structured and unstructured data and is, therefore, not suitable for extensive analytics on such data. For migration between two RDBMSs, schemas and type of data need to be identical between the source and destination tables. Relational databases have a limit on the length of data fields, which means if you try to enter more information into a field than it can accommodate, the information will not be stored.

Despite the limitations and the evolution of data in these times of big data, cloud computing, IoT devices, and social media, RDBMS continues to be the predominant technology for working with structured data.

## NoSQL
NoSQL, which stands for “not only SQL,” or sometimes “non SQL” is a non-relational database design that provides flexible schemas for the storage and retrieval of data. NoSQL databases have existed for many years but have only recently become more popular in the era of cloud, big data, and high-volume web and mobile applications. They are chosen today for their attributes around scale, performance,and ease of use. It's important to emphasize that the "No" in "NoSQL" is an abbreviation for "not only" and not the actual word "No."

NoSQL databases are built for specific data models and have flexible schemas that allow programmers to create and manage modern applications. They do not use a traditional row/column/table database design with fixed schemas, and typically not use the structured query language (or SQL) to query data, although some may support SQL or SQL-like interfaces. NoSQLallows data to be stored in a schema-less or free-form fashion. Any data, be It structured, semi-structured, or unstructured,can be stored in any record. Based on the model being used for storing data, there are four common types of NoSQL databases: Key-value store, Document-based, Column-based, and graph-based.

- Key-value store
	Data in a key-value database is stored as a collection of key-value pairs. The key represents an attribute of the data and is a unique identifier. Both keys and values can be anything from simple integers or strings to complex JSON documents. Key-value stores are great for storing user session dataanduser preferences, making real-time recommendations and targeted advertising, and in-memory data caching. However, if you want to be able to query the data on specific data value, need relationships between data values, or need to have multiple unique keys, a key-value store may not be the best fit. Redis, Memcached, and DynamoDB are some well-known examples in this category.

- Document-based
	Document databases store each record and its associated data within a single document. They enable flexible indexing, powerful ad hoc queries, and analytics over collections of documents. Document databases are preferable for eCommerce platforms, medical records storage, CRM platforms, and analytics platforms. However, if you’re looking to run complex search queries and multi-operation transactions, a document-based database may not be the best option for you. MongoDB, DocumentDB, CouchDB, and Cloudant are some of the popular document-based databases.
	
- Column-based
	Column-based models store data in cells grouped as columns of data instead of rows. A logical grouping of columns, that is, columns that are usually accessed together, is called a column family. For example, a customer’s name and profile information will most likely be accessed together but not their purchase history. So, customer name and profile information data can be grouped into a column family. Since column databases store all cells corresponding to a column as a continuous disk entry, accessing and searching the data becomes very fast. Column databases can be great for systems that require heavy write requests, storing time-series data, weather data, and IoT data. But if you need to use complex queries or change your querying patterns frequently, this may not be the best option for you. The most popular column databases are Cassandra and HBase.
	
- Graph-based
	Graph-based databases use a graphical model to represent and store data. They are particularly useful for visualizing, analyzing, and finding connections between different pieces of data. The circles arenodes, and they contain the data. The arrows represent relationships. Graph databases are an excellent choice for working with connected data, which is data that contains lots of interconnected relationships. Graph databases are great for social networks, real-time product recommendations, network diagrams, fraud detection, and access management. But if you want to process high volumes of transactions, it may not be the best choice for you, because graph databases are not optimized for large-volume analytics queries. Neo4J and CosmosDB are some of the more popular graph databases.
	
### Advantages
NoSQL was created in response to the limitations of traditional relational database technology. 
- The primary advantage is its ability to handle large volumes of structured, semi-structured, and unstructured data.
- The ability to run as distributed systems scaled across multiple data centers, which enables them to take advantage of cloud computing infrastructure;
- An efficient and cost-effective scale-out architecture that provides additional capacity and performance with the addition of new nodes;
- Simpler design, better control over availability, and improved scalability that enables you to be more agile, more flexible, and to iterate more quickly.

The key differences between relational and non-relational databases: RDBMS schemas rigidly define how all data inserted into the database must be typed and composed, whereas NoSQL databases can be schema-agnostic, allowing unstructured and semi-structured data to be stored and manipulated. Maintaining high-end, commercial relational database management systems is expensive whereas NoSQL databases are specifically designed for low-cost commodity hardware Relational databases, unlike most NoSQL, support ACID-compliance, which ensures reliability of transactions and crash recovery. RDBMS is a mature and well-documented technology, which means the risks are more or less perceivable as compared to NoSQL, which is a relatively newer technology.

## Data Marts, Data Lakes, ETL and Data Pipelines

### Types of Data Repositories
A **data warehouse** works like a multi-purpose storage for different use cases. By the time the data comes into the warehouse, it has already been modeled and structured for a specific purpose, meaning it is analysis ready. As an organization, you would opt for a data warehouse when you have massive amounts of data from your operational systems that needs to be readily available for reporting and analysis. Data warehouses serve as the single source of truth—storing current and historical data that has been cleansed, conformed, and categorized. A data warehouse is a multi-purpose enabler of operational and performance analytics.

**Data mart** is a sub-section of the data warehouse, built specifically for a particular business function, purpose, or community of users. The idea is to provide stakeholders data that is most relevant to them, when they need it. For example, the sales or finance teams accessing data for their quarterly reporting and projections. Since a data mart offers analytical capabilities for a restricted area of the data warehouse, it offers isolated security and isolated performance. The most important role of a data mart is business-specific reporting and analytics.

**Data Lake** is a storage repository that can store large amounts of structured, semi-structured, and unstructured data in their native format, classified and tagged with metadata. So, while a data warehouse stores data processed for a specific need, a data lake is a pool of raw data where each data element is given a unique identifier and is tagged with metatags for further use. You would opt for a data lake if you generate, or have access to, large volumes of data on an ongoing basis, but don’t want to be restricted to specific or pre-defined use cases. Unlike data warehouses, a data lake would retain all source data, without any exclusions. And the data could include all types of data sources and types. Data lakes are sometimes also used as a staging area of a data warehouse. The most important role of a data lake is in predictive and advanced analytics. 


### Data Pipelines
**ETL** is how raw data is converted into analysis-ready data. It is an automated process in which you gather raw data from identified sources, extract the information that aligns with your reporting and analysis needs, clean, standardize, and transform that data into a format that is usable in the context of your organization; and load it into a data repository. While ETL is a generic process, the actual job can be very different in usage, utility, and complexity.

**Extract** is the step where data from source locations is collected for transformation. Data extraction could be through: Batch processing, meaning source data, is moved in large chunks from the source to the target system at scheduled intervals. Tools for batch processing include Stitch and Blendo. Stream processing, which means source data is pulled in real-time from the source and transformed while it is in transit and before it is loaded into the data repository. Tools for stream processing include Apache Samza, Apache Storm, and Apache Kafka.

**Transform** involves the execution of rules and functions that converts raw data into data that can be used for analysis. For example, making date formats and units of measurement consistent across all sourced data, removing duplicate data, filtering out data that you do not need, enriching data, for example, splitting full name to first, middle, and last names, establishing key relationships across tables, applying business rules and data validations. 

**Load** is the step where processed data is transported to a destination system or data repository. It could be: Initial loading, that is, populating all the data in the repository, Incremental loading, that is, applying ongoing updates and modifications as needed periodically; or Full refresh, that is, erasing contents of one or more tables and reloading with fresh data. Load verification, which includes data checks for missing or null values, server performance, and monitoring load failures, are important parts of this process step. It is vital to keep an eye on load failures and ensure the right recovery mechanisms are in place.

ETL has historically been used for batch workloads on a large scale. However, with the emergence of streaming ETL tools, they are increasingly being used for real-time streaming event data as well. It’s common to see the terms ETL and data pipelines used interchangeably. And although both move data from source to destination, data pipeline is a broader term that encompasses the entire journey of moving data from one system to another, of which ETL is a subset. 

**Data pipelines** can be architected for batch processing, for streaming data, and a combination of batch and streaming data. In the case of streaming data, data processing or transformation, happens in a continuous flow. This is particularly useful for data that needs constant updating, such as data from a sensor monitoring traffic. A data pipeline is a high performing system that supports both long-running batch queries and smaller interactive queries. The destination for a data pipeline is typically a data lake, although the data may also be loaded to different target destinations, such as another application or a visualization tool. There are a number of data pipeline solutions available, most popular among them being Apache Beam and DataFlow.

## Viewpoints: Considerations for Choice of Data Repository

You need to look at the use case. What is the data repository going to be used for? Is it going to be used for storing structured information, semi-structured or unstructured information. Or do you know beforehand what the schema of the data is? Is there performance requirements? Are you working with data at rest, or streaming data, or data in motion? Does the data need to be encrypted? Does there... is there, you know, what's the volume of data that you're working with? Do you need a big data system? And what are the storage requirements? Does the data need to be updated frequently and accessed frequently, that it just needs to be stored and kept in a valt for a long time and is needed for backup purposes for example? And then your organization might have certain standards that might have put in place of which databases or which data repositories you're allowed to use for different kinds of tasks. So all of these factors need to be kept in mind.

So when we consider what data repository we want to choose, we look at these factors. We look at what are the kind of capacities that this data repository is supposed to handle. And then we also look at the type of access that we need this for. Do we access it in short intervals or do we run long running queries on it? Am I using it more for transaction processing or am I using it for analytics or archival purposes, or for data warehousing purpose? We also look for compatibility. How compatible this new data repository is with my existing ecosystem of programming languages, tools, and any processes that we have. We also consider the security features this repository gives us. And the most important thing is scalability. We may be happy with its performance today, but is it scalable enough? Can it scale along with the organization?

I don't often get to choose the type of data repository that my organization uses, and very few organizations use one data repository these days. On my team that I work on these days, we have a set of preferred solutions. We have a preferred enterprise relational database. We have a preferred open-source relational database for some of the smaller projects and for the microservices. And then we also have a preferred unstructured data source. So those are three main ones. The important thing is to think about the skills that you have within your organization or that you want to foster within your organization. And consider the costs of the various solutions. In our case, we have some experts on Db2, so our enterprise database of choice is Db2. However, there are other projects that use different ones. For open source, we've changed that a couple of times. We've got a couple of different directions with where we really want to be there. And all of these... the hosting platform makes a difference as well, because now it's not just do I want to use IBM Db2 or do I want to use some other vendors, Microsoft SQL Server or whatever. It's not between those two choices. It's when I do those, do I want to do them on AWS RDS? Maybe I should consider Amazon's Aurora. Maybe I should consider Googles relational offerings. There's so many different choices there that you have to consider. There's the decision of how should the data be stored. There's the decision of how should the data be retrieved, and there's also the decision of where. Those are all very important questions when you're deciding on data storage.

I would say the structure of the data, the nature of the application, and the volume at which the data is getting ingested into your database, all these factors determine the nature of the data source that you should pick. In most cases a relational database should be enough, however, there will be edge cases where relational databases such as IBM Db2, Oracle or Postgres won't necessarily do the job for you. In those cases, so depending on the use case, for example, if you are ingesting gigabytes or terabytes of data per day. then document stores such as MongoDB, or wide column stores such as Cassandra might be a good fit for you. At the same time, if you're trying to build a product recommendation engines or trying to show the network of relationships between different people on the social media, then graph data structures such as Neo4J or Apache TinkerPop would be an ideal fit for you. At the same time, if you are mining through terabytes or petabytes of data for analytics, Hadoop engine with MapReduce would be a good fit for you. So it really boils down to the nature of the application and the volume of the data, and the structure of the data, before you can pick the right database or data source whatever the use case.

## Data Integration Platforms

Gartner **defines** data integration as a discipline comprising the practices, architectural techniques, and tools that allow organizations to ingest, transform, combine, and provision data across various data types. The report further explains that data integration has several usage scenarios, such as data consistency across applications, master data management, data sharing between enterprises, and data migration and consolidation.

In the field of analytics and data science, **data integration** includes accessing, queueing, or extracting data from operational systems transforming and merging extracted data either logically or physically data quality and governance, and delivering data through an integrated approach for analytics purposes For example, to make customer data available for analytics, you would need to extract individual customers' information from operational systems such as sales, marketing, and finance. You would then need to provide a unified view of the combined data so that your users can access, query, and manipulate this data from a single interface to derive statistics, analytics, and visualizations. 

### How does a data integration platform relate to ETL and data pipelines?
While data integration combines disparate data into a unified view of the data, a data pipeline covers the entire data movement journey from source to destination systems. In that sense, you use a data pipeline to perform data integration, while ETL is a process within data integration.

There is no one approach to data integration. However, modern data integration solutions typically support the following capabilities:
- An extensive catalog of pre-built connectors and adopters that help you connect and build integration flows with a wide variety of data sources such as databases, flat files, social media data, APIs, CRM and ERP applications. 
- Open-source architecture that provides greater flexibility and avoids vendor lock-in.
- Optimization for both batch processing of large-scale data and continuous data streams, or both.
- Integration with Big Data sources. Support for big data is increasingly driving the decision regarding choice of integration platforms.
- Additional functionalities. For example, specific demands around data quality and governance, compliance, and security.
- Portability, which ensures that as businesses move to cloud models, they should be able to run their data integration platforms anywhere. And data integration tools are able to work natively in a single cloud, multi-cloud, or hybrid cloud environment.

There are many data integration platforms and tools available in the market, ranging from commercial off-the-shelf tools to open-source frameworks. IBM offers a host of data integration tools targeting a range of enterprise integration scenarios, such as Information Server for IBM, Cloud Pak for Data, IBM Cloud Pak for Integration, IBM Data Replication, IBM Data Virtualization Manager, IBM InfoSphere Information Server on Cloud, and IBM InfoSphere DataStage all target a range of enterprise data integration scenarios. Talend's data integration tools include Talend Data Fabric, Talend Cloud, Talend Data Catalog, Talend Data Management, Talend Big Data, Talend Data Services, and Talend Open Studio. SAP, Oracle, Denodo, SAS, Microsoft, Qlik, and TIBCO are some of the other vendors that offer data integration tools and platforms. Examples of open-source frameworks include Dell Boomi, Jitterbit, and SnapLogic. There are a significant number of vendors who are offering cloud-based Integration Platform as a Service, or iPaaS, as a hosted service via virtual private cloud or hybrid cloud. Such as the Adeptia Integration Suite, Google Cloud's Cooperation 534, IBM's Application Integration Suite on Cloud, and Informatica's Integration Cloud.

The data integration space continues to evolve as businesses embrace newer technologies and as data grows, be it in the variety of sources or its use in business decision-making.

## Lesson Summary

As a data scientist, you need an awareness of data storage possibilities for its organization and management, and options for retrieval. These systems enable you to find and analyze the data you need to make great discoveries hidden in that data.

Let's consider data repositories. These repositories need the ability to find the data you want and return it to you in a usable format. Your data type helps determine the type of repository you need. You can store structured, semi structured or unstructured data. Depending on the organization, you may need a relational or no SQL database. For big data stores, your needs may call for a data warehouse, a data mart, or a data lake.

Relational databases store structured data. These are the oldest types of repositories. The most conventional and frequently used relational database management systems, often abbreviated as RDBMSs, are based on the foundational concept of structuring data in a tabular format with data arranged in rows and columns. Each table usually relates to a topic, and the columns of data in the table contain a specific type of information related to that topic. Then the database contains a defined schema that describes the table to each other. Relational databases usually rely on structured query language or SQL, to search for and retrieve the data you need. You use SQL to manipulate the data. Relational databases are beneficial for visualizing, analyzing, and finding connections between different pieces of data. You link tables together by creating schemas, you can restrict database fields to specific data types and values which minimizes irregularities and leads to greater consistency and data integrity. They offer easy export and import options, making back up and restoration easy. However, RBMS is do not work well with semi structured or unstructured data. They are also slow to query with enormous datasets. Since RDBMSs use predefined structures for data to reside in, it becomes problematic when the data evolves and no longer conforms to that structure. Relational databases also limit field length, which means that sometimes they cannot accommodate the information you need. Because of these limitations and the quantities and diversity of data collected, many organizations have turned to not only SQL databases, or no SQ L for short. Built for speed, flexibility, and scale, non relational databases allow storing data without stringent schemas. They can house semi structured and unstructured data. No SQL databases include document based, key value, columnar and graph. Document based databases store semi structured documents, such as Jason files. You group documents into collections, and each document has its structure. Key value stores each piece of data as a key value pair, so you retrieve and update the data using the key. Columnar databases store data and columns rather than rows, enabling storage of large volumes of data suitable for analytical workloads. Graph databases store data in nodes. Nodes have relationships and properties, and can manage and query complex relationships between them. You can use technologies such as data warehouses, data marts, and data lakes for high volumes of data. A data warehouse works like a multipurpose storage for different use cases. The data has already been modeled and structured for a specific purpose. As an organization, you would opt for a data warehouse when you have a massive amount of data from your operational systems that must be readily available for reporting and analysis. A data mart is a subsection of the data warehouse built specifically for a particular business function, purpose, or community of users. A data mart offers analytical capabilities for restricted data warehouse area, offering isolated security and performance. A data lake is a storage repository that can store large amounts of structured, semi structured, and unstructured data in their native format, classified and tagged with meta data. Let's review storage options. Data pipelines address an organization's need to collect, transform, and move data. Data pipelines have multiple steps, providing a systematic process to handle massive amounts of data as it is continually collected, processed, and made available. ETL, which stands for extract, transform, and load, is a subset of a data pipeline, referring to an automated process where an organization converts its raw data into data ready for analysis. Now as a future data scientist, you are aware of many technologies needed to handle big data before analysis can begin. These include data storage, organization and management and retrieval. Data storage options depend on the type of data, its volume, and how you intend to organize it. Using a data pipeline such as ETL, provides a process to manage and retrieve the data so you can analyze it as a data scientist.
