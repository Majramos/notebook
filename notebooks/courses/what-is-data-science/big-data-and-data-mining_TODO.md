# Big Data and Data Mining

## How Big Data is Driving Digital Transformation

Digital Transformation affects business operations, updating existing processes and operations and creating new ones to harness the benefits of new technologies.
This digital change integrates digital technology into all areas of an organization resulting in fundamental changes to how it operates and delivers value to customers. It is an organizational and cultural change driven by Data Science, and especially Big Data. 
The availability of vast amounts of data, and the competitive advantage that analyzing it brings, has triggered digital transformations throughout many industries.

### Example
Netflix moved from being a postal DVD lending system to one of the world’s foremost video streaming providers, the Houston Rockets NBA team used data gathered by overhead cameras to analyze the most productive plays, and Lufthansa analyzed customer data to improve its service.
Organizations all around us are changing to their very core. Let’s take a look at an example, to see how Big Data can trigger a digital transformation, not just in one organization, but in an entire industry.
In 2018, the Houston Rockets, a National Basketball Association, or NBA team, raised their game using Big Data. The Rockets were one of four NBA teams to install a video tracking system which mined raw data from games. They analyzed video tracking data to investigate which plays provided the best opportunities for high scores, and discovered something surprising. Data analysis revealed that the shots that provide the best opportunities for high scores are two-point dunks from inside the two-point zone, and three-point shots from outside the three-point line, not long-range two-point shots from inside it. This discovery entirely changed the way the team approached each game, increasing the number of three-point shots attempted. In the 2017-18 season, the Rockets made more three-point shots than any other team in NBA history, and this was a major reason they won more games than any of their rivals. In basketball, Big Data changed the way teams try to win, transforming the approach to the game.

### Summary
Digital transformation is not simply duplicating existing processes in digital form; the in-depth analysis of how the business operates helps organizations discover how to improve their processes and operations, and harness the benefits of integrating data science into their workflows. Most organizations realize that digital transformation will require fundamental changes to their approach towards data, employees, and customers, and it will affect their organizational culture. Digital transformation impacts every aspect of the organization, so it is handled by decision makers at the very top levels to ensure success. The support of the Chief Executive Officer is crucial to the digital transformation process, as is the support of the Chief Information Officer, and the emerging role of Chief Data Officer. But they also require support from the executives who control budgets, personnel decisions, and day-to-day priorities. This is a whole organization process. Everyone must support it for it to succeed. There is no doubt dealing with all the issues that arise in this effort requires a new mindset, but Digital Transformation is the way to succeed now and in the future.


## Introduction to Cloud

Cloud computing, also referred to as the cloud, is the delivery of on-demand computing resources such as networks, servers, storage, applications, services, and data centers over the Internet on a pay-for-use basis. The term “cloud computing” can be used to describe applications and data that users access over the Internet rather than on their local computer.

Examples of cloud computing include users using online web apps, employees using secure online business applications to conduct their work, and users storing personal files on cloud-based storage platforms such as Google Drive, OneDrive, and Dropbox.

### Benefits
One of the main user benefits of cloud computing is that instead of users needing to purchase their own applications and install them locally on their computer, they can **use online versions** of those applications and pay a monthly subscription. Not only is this typically more cost-effective initially, but users can also access the **latest version of the application** without having to purchase a full retail copy of the newer version. A side advantage of this is that the user also saves lots of local storage space as the application is hosted online. And, the beauty of most cloud-based applications is that they also enable users to work collaboratively with their colleagues, working on the same files in real time and being able to see each other’s edits and updates.

Cloud computing is composed of five essential characteristics, three deployment models, and three service models.

### Characteristics

- On-demand self-service means that you get access to cloud resources such as the processing power, storage, and network you need, using a simple interface, without requiring human interaction with each service provider.
- Broad network access means that cloud computing resources can be accessed via the network through standard mechanisms and platforms such as mobile phones, tablets, laptops, and workstations.
- Resource pooling is what gives cloud providers economies of scale, which they pass on to their customers, making cloud cost-efficient.
- Using a multitenant model, computing resources are pooled to serve multiple consumers, and cloud resources are dynamically assigned and reassigned according to demand, without customers needing to know the physical location of these resources.
- Rapid elasticity implies that you can access more resources when you need them, and scale back when you don’t, because resources are elastically provisioned and released. 
- Measured service means that you only pay for what you use or reserve as you go. If you’re not using resources, you’re not paying. Resource usage is monitored, measured, and reported transparently based on consumer utilization.

As you have seen, cloud computing is really about using technology “as a service,” leveraging remote systems on-demand over the open Internet, scaling up and scaling back, and paying for what you use. And it has changed the way the world consumes compute services, by making them more cost-efficient while also making organizations more agile in response to changes in their markets.

### Deployment Models
Cloud deployment models indicate where the infrastructure resides, who owns and manages it, and how cloud resources and services are made available to users. There are three types of cloud deployment models: public, private, and hybrid. 
- Public cloud is when you leverage cloud services over the open internet on hardware owned by the cloud provider, but its usage is shared by other companies.
- Private cloud means that the cloud infrastructure is provisioned for exclusive use by a single organization. It could run on-premises or it could be owned, managed, and operated by a service provider.
- And when you use a mix of both public and private clouds, working together seamlessly, that is classified as the hybrid cloud model.

### Service Models
Now, let’s look at the three cloud service models that are based on the three layers in a computing stack: infrastructure, platform, and application.

These cloud computing models are aptly referred to as
- Infrastructure as a Service (or IaaS)
In an IaaS model, you can access the infrastructure and physical computing resources such as servers, networking, storage, and data center space without the need to manage or operate them.
- Platform as a Service (or PaaS)
In a PaaS model, you can access the platform that comprises the hardware and software tools that are usually needed to develop and deploy applications to users over the Internet.
- Software as a Service (or SaaS)
In a SaaS is a software licensing and delivery model in which software and applications are centrally hosted and licensed on a subscription basis. It is sometimes referred to as “on-demand software.”

### Summary
Cloud computing is the delivery of on-demand computing resources over the Internet on a pay-for-use basis. Cloud computing is composed of five essential characteristics, three deployment models, and three service models. The five essential characteristics of cloud computing are on-demand self-service, broad network access, resource pooling, rapid elasticity, and measured service. There are three types of cloud deployment models: public, private, and hybrid. And the three cloud service models are based on the three layers in a computing stack (infrastructure, platform, and application), and they are referred to as Infrastructure as a Service (or IaaS), Platform as a Service (or PaaS), and Software as a Service (or SaaS)

## Cloud for Data Science
Cloud is a godsend for data scientists. Primarily because you're able to take your data, take your information and put it in the Cloud, put it in a central storage system.

It allows you to bypass the physical limitations of the computers and the systems you're using and it allows you to deploy the analytics and storage capacities of advanced machines that do not necessarily have to be your machine or your company's machine.

Cloud allows you not just to store large amounts of data on servers somewhere in California or in Nevada, but it also allows you to deploy very advanced computing algorithms and the ability to do high-performance computing using machines that are not yours.

Think of it as you have some information, you can't store it, so you send it to storage space, let's call it Cloud, and the algorithms that you need to use, you don't have them with you. But then on the Cloud, you have those algorithms available. So What you do is you deploy those algorithms on very large datasets and you're able to do it even though your own systems, your own machines, your own computing environments were not allowing you to do so.

So Cloud is beautiful. The other thing that Cloud is beautiful for is that it allows multiple entities to work with same data at the same time. You can be working with the same data that your colleagues in say Germany and another team in India and another team in Ghana, they are collectively working and they're able to do so because the information, and the algorithms, and the tools, and the answers, and the results, whatever they needed is available at a central place, which we call Cloud. Cloud is beautiful.

### Summary
Using the Cloud enables you to get instant access to open source technologies like Apache Spark without the need to install and configure them locally. Using the Cloud also gives you access to the most up-to-date tools and libraries without the worry of maintaining them and ensuring that they are up to date.

The Cloud is accessible from everywhere and in every time zone. You can use cloud-based technologies from your laptop, from your tablet, and even from your phone, enabling collaboration more easily than ever before. Multiple collaborators or teams can access the data simultaneously, working together on producing a solution.

Some big tech companies offer Cloud platforms, allowing you to become familiar with cloud-based technologies in a pre-built environment. IBM offers the IBM Cloud, Amazon offers Amazon Web Services or AWS, and Google offers Google Cloud platform. IBM also provides Skills Network labs or SN labs to learners registered at any of the learning portals on the IBM Developer Skills Network, where you have access to tools like Jupyter Notebooks and Spark clusters so you can create your own data science project and develop solutions. With practice and familiarity, you will discover how the Cloud dramatically enhances productivity for data scientists.

## Foundations of Big Data

In this digital world, everyone leaves a trace. From our travel habits to our workouts and entertainment, the increasing number of internet connected devices that we interact with on a daily basis record vast amounts of data about us. There’s even a name for it: Big Data.

Ernst and Young offers the following definition: “Big Data refers to the dynamic, large and disparate volumes of data being created by people, tools, and machines. It requires new, innovative, and scalable technology to collect, host, and analytically process the vast amount of data gathered in order to derive real-time business insights that relate to consumers, risk, profit, performance, productivity management, and enhanced shareholder value.”


There is no one definition of Big Data, but there are certain elements that are common across the different definitions, such as **velocity, volume, variety, veracity, and value. These are the V's of Big Data.**

### V's of Big Data
- Velocity 
The speed at which data accumulates. Data is being generated extremely fast, in a process that never stops. Near or real-time streaming, local, and cloud-based technologies can process information very quickly.

example: Every 60 seconds, hours of footage are uploaded to YouTube which is generating data. Think about how quickly data accumulates over hours, days, and years

- Volume
The scale of the data, or the increase in the amount of data stored. Drivers of volume are the increase in data sources, higher resolution sensors, and scalable infrastructure.

example: The world population is approximately seven billion people and the vast majority are now using digital devices; mobile phones, desktop and laptop computers, wearable devices, and so on. These devices all generate, capture, and store data -- approximately 2.5 quintillion bytes every day. That's the equivalent of 10 million Blu-ray DVD's

- Variety
The diversity of the data. Structured data fits neatly into rows and columns, in relational databases while unstructured data is not organized in a pre-defined way, like Tweets, blog posts, pictures, numbers, and video. Variety also reflects that data comes from different sources, machines, people, and processes, both internal and external to organizations. Drivers are mobile technologies, social media, wearable technologies, geo technologies, video, and many, many more.

example: Let's think about the different types of data; text, pictures, film, sound, health data from wearable devices, and many different types of data from devices connected to the Internet of Things. 

- Veracity
The quality and origin of data, and its conformity to facts and accuracy. Attributes include consistency, completeness, integrity, and ambiguity. Drivers include cost and the need for traceability. With the large amount of data available, the debate rages on about the accuracy of data in the digital age. Is the information real, or is it false?

example: 80% of data is considered to be unstructured and we must devise ways to produce reliable and accurate insights. The data must be categorized, analyzed, and visualized.

- Value 
Ability and need to turn data into value. Value isn't just profit. It may have medical or social benefits, as well as customer, employee, or personal satisfaction. The main reason that people invest time to understand Big Data is to derive value from it.

### Summary
Data Scientists today derive insights from Big Data and cope with the challenges that these massive data sets present. The scale of the data being collected means that it’s not feasible to use conventional data analysis tools. However, alternative tools that leverage distributed computing power can overcome this problem. Tools such as Apache Spark, Hadoop and its ecosystem provide ways to extract, load, analyze, and process the data across distributed compute resources, providing new insights and knowledge. This gives organizations more ways to connect with their customers and enrich the services they offer. So next time you strap on your smartwatch, unlock your smartphone, or track your workout, remember your data is starting a journey that might take it all the way around the world, through big data analysis, and back to you.

## Data Science and Big Data (Norman White)
Everybody knows how to program, at least a little bit. They all have a little bit of programming background at least, and some of them have a lot. Some of them are Masters of Science and Computer Science, some of them are MBA students who've come in from technical fields and programmed every day. And others are ones who maybe took a programming course in college four or five years ago but at least they can think computationally, which I think is the most important thing that they need. (music)

Data science and business analytics have become very hot subjects in the last four or five years. We have new tools, we have new approaches, and we have lots and lots of data that traditional techniques just couldn't really store and handle. I think the word is out. I think at this point, at first, companies and employers understood the need, especially in certain fields. I can remember talking to a major bank three years ago about big data and there was one little group in the bank where one person had a little effort in putting a little cluster together. Now that same bank has five or six major big data clusters and they're putting all of their credit card data in it and they're grinding it upside down and sideways, using all sorts of data science kinds of techniques.

Two years ago, or was it last year, I think, our undergraduate dealing with data course had 28 students in it. This year it has 140. So that means that the parents are now beginning to get the word, because one thing we understand with our undergrads is the parents who are paying very hefty tuitions, they, you know, they tell their sons and daughters, "You know, you should be an accountant," right? Or, "You should go into financial services, "or into marketing, 'cause this is where the money is." Now, they're getting the word that maybe you should take some more STEM classes in high school and be ready to go into data science or go into fields where analytics has become more and more important.

It depends on who you are. I have my own definition of big data. My definition of big data is data that is large enough and has enough volume and velocity that you cannot handle it with traditional database systems. Some of our statisticians think big data is something you can't fit on a thumb drive. **Big data, to me, was started by Google.** When Google tried to figure out how they were, when Larry Page and Sergey Brin wanted to, basically, figure out how to solve their page rank algorithm, there was nothing out there. They were trying to store all of the web pages in the world, and there was no technology, there was no way to do this, and so they went out and developed this approach, which has now become, Hadoop has copied it, but this is where all these large, big data clusters are found. But big data has now also expanded into, how do you analyze? There are new analytical techniques and statistical techniques for handling these really, really, really large data sets. We'll probably get to deep learning at some point along here.

## What is Hadoop? (Norman White)

Traditionally in computation and processing data we would bring the data to the computer. You'd wanna program and you'd bring the data into the program. In a big data cluster what Larry Page and Sergey Brin came up with is very pretty simple is they **took the data and they sliced it into pieces and they distributed each** and they replicated each piece or triplicated each piece and they would send it the pieces of these files to thousands of computers first it was hundreds but then now it's thousands now it's tens of thousands. And then they would send the same program to all these computers in the cluster. And each computer would run the program on its little piece of the file and send the results back. The results would then be sorted and those results would then be redistributed back to another process. The first process is called a map or a mapper process and the second one was called a reduce process. Fairly simple concepts but turned out that you could do lots and lots of different kinds of handle lots and lots of different kinds of problems and very, very, very large data sets. So the one thing that's nice about these **big data clusters is they scale linearly.** You had twice as many servers and you get twice the performance and you can handle twice the amount of data. So this was just broke a bottleneck for all the major social media companies. Yahoo then got on board. Yahoo hired someone named Doug Cutting who had been working on a clone or a copy of the Google big data architecture and now that's called Hadoop. And if you google Hadoop you'll see that it's now a very popular term and there are many, many, many if you look at the big data ecology there are hundreds of thousands of companies out there that have some kind of footprint in the big data world.

Most of the **components of data science** have been around for many, many, many, many decades. But they're all coming together now with some new nuances I guess. At the bottom of data science you see probability and statistics. You see algebra, linear algebra you see programming and you see databases. They've all been here. But what's happened now is we now have the computational capabilities to apply some new techniques - machine learning. Where now we can take really large data sets and instead of taking a sample and trying to test some hypothesis we can take really, really large data sets and look for patterns. And so back off one level from hypothesis testing to finding patterns that maybe will generate hypotheses. Now this can bother some very traditional statisticians and gets them really annoyed sometimes that you know you're supposed to have a hypothesis that is not that is independent of the data and then you test it. So once some of these machine learning techniques started were really the only thing the only way you can analyze some of these really large social media data sets. 

So what we've seen is that the combination of traditional areas computer science probability, statistics, mathematics all coming together in this thing that we call **Decision Sciences**. Our department at Stern I'll give a little plug here we happen to have been very well situated among business schools because we're one of the few business schools that has a real statistics department with real PhD level statisticians in it. We have an operations management department and an information systems department. So we have a wide range of computer scientists to statisticians, to operations researchers. And so we were like perfectly positioned as a couple of other business schools were to jump on this bandwagon and say; okay this is Decision Sciences. And Foster Provost who's in my department was the first director of the NYU Center for Data Science.

Four years ago maybe five years ago. I mean, I feel this is one of those cases where you can just to Google and search for data science and see how often it occurred and you'll see almost nothing and then just a spike. The same thing you would see with big data about seven or eight years ago. 
So **data science** is a term I haven't heard of probably five years ago.
The first question is **what is it?** And I think faculty and everybody is still trying to get their hands around exactly what is business analytics and what is data science. We certainly know the components of it. But it's morphing and changing and growing. I mean the last three years deep learning has just been added into the mix. Neural networks have been around for 20 or 30 years. 20 years ago, I would teach neural networks in a class and you really couldn't do very much with them. And now some researchers have come up with multi-layer neural networks in Toronto in particular the University of Toronto. And that technology is now rapidly expanding it's being used by Google, by Facebook, by lots of companies.

## Big Data Processing Tools: Hadoop, HDFS, Hive, and Spark

The Big Data processing technologies provide ways to work with large sets of structured, semi-structured, and unstructured data so that value can be derived from big data.
In some of the other videos, we discussed Big Data technologies such as NoSQL databases and Data Lakes. In this video, we are going to talk about three open source technologies and the role they play in big data analytics — ApacheHadoop, Apache Hive, and Apache Spark.

### Hadoop
- Collection of tools that provides distributed storage and processing of big data. Hadoop does involve data storage, but it is primarily known for data analysis and   processing, not just storage.

Hadoop, a java-based open-source framework, allows distributed storage and processing of large datasets across clusters of computers. In Hadoop distributed system, a node is a single computer, and a collection of nodes forms a cluster. Hadoop can scale up from a single node to any number of nodes, each offering local storage and computation. Hadoop provides a reliable, scalable, and cost-effective solution for storing data with no format requirements. 

Using Hadoop, you can: 
- Incorporate emerging data formats, such as streaming audio, video, social media sentiment, and clickstream data, along with structured, semi-structured, and unstructured data not traditionally used in a data warehouse. 
- Provide real-time, self-service access for all stakeholders.
- Optimize and streamline costs in your enterprise data warehouse by consolidating data across the organization and moving “cold” data, that is, data that is not in frequent use, to a Hadoop-based system.

One of the four main components of Hadoop is Hadoop Distributed File System, or HDFS, which is a storage system for big data that runs on multiple commodity hardware connected through a network. HDFS provides scalable and reliable big data storage by partitioning files over multiple nodes. It splits large files across multiple computers, allowing parallel access to them. **Computations can, therefore, run in parallel on each node where data is stored**. It also replicates file blocks on different nodes to prevent data loss, making it **fault-tolerant**.

Let’s understand this through an example. Consider a file that includes phone numbers for everyone in the United States; the numbers for people with last name starting with A might be stored on server 1, B on server 2, and so on. With Hadoop, pieces of this phonebook would be stored across the cluster. To reconstruct the entire phonebook, your program would need the blocks from every server in the cluster. HDFS also replicates these smaller pieces onto two additional servers by default, ensuring availability when a server fails, In addition to higher availability, this offers multiple benefits. It allows the Hadoop cluster to break up work into smaller chunks and run those jobs on all servers in the cluster for **better scalability**.

Finally, you gain the benefit of **data locality**, which is the process of moving the computation closer to the node on which the data resides. This is critical when working with large data sets because it minimizes network congestion and increases throughput. Some of the other benefits that come from using HDFS include: Fast recovery from hardware failures, because HDFS is built to detect faults and automatically recover.
Access to streaming data, because HDFS supports high data throughput rates. Accommodation of large data sets, because HDFS can scale to hundreds of nodes, or computers, in a single cluster. **Portability**, because HDFS is portable across multiple hardware platforms and compatible with a variety of underlying operating systems.

### Hive
- Data warehouse for data query and analysis built on top of Hadoop.

- Hive is an open-source data warehouse software for reading, writing, and managing large data set files that are stored directly in either HDFS or other data storage systems such as Apache HBase.
- Hadoop is intended for long sequential scans and, because Hive is based on Hadoop, queries have very high latency—which means Hive is less appropriate for applications that need very fast response times.
- Hive is read-based, and therefore not suitable for transaction processing that typically involves a high percentage of write operations.
- Hive is better suited for data warehousing tasks such as ETL, reporting, and data analysis and includes tools that enable easy access to data via SQL.

### Spark
- Distributed data analytics framework designed to perform complex data analytics in real-time.
- A general-purpose data processing engine designed to extract and process large volumes of data for a wide range of applications, including Interactive Analytics, Streams Processing, Machine Learning, Data Integration, and ETL.
- It takes advantage of in-memory processing to significantly increase the speed of computations and spilling to disk only when memory is constrained.
- Spark has interfaces for major programming languages, including Java, Scala, Python, R, and SQL. It can run using its standalone clustering technology as well as on top of other infrastructures such as Hadoop. And it can access data in a large variety of data sources, including HDFS and Hive, making it highly versatile.
- The ability to process streaming data fast and perform complex analytics in real-time is the key use case for Apache Spark.


## Data Mining

### Establishing Data Mining Goals
The first step in data mining requires you to set up goals for the exercise. Obviously, you must identify the key questions that need to be answered. However, going beyond identifying the key questions are the concerns about the costs and benefits of the exercise. Furthermore, you must determine, in advance, the **expected level of accuracy and usefulness** of the results obtained from data mining. If money were no object, you could throw as many funds as necessary to get the answers required. However, the cost-benefit trade-off is always instrumental in determining the goals and scope of the data mining exercise. The level of accuracy expected from the results also influences the costs. High levels of accuracy from data mining would cost more and vice versa. Furthermore, beyond a certain level of accuracy, you do not gain much from the exercise, given the diminishing returns. Thus, the cost-benefit trade-offs for the desired level of accuracy are important considerations for data mining goals.

### Selecting Data
The output of a data-mining exercise largely depends upon the quality of data being used. At times, data are readily available for further processing. For instance, retailers often possess large databases of customer purchases and demographics. On the other hand, data may not be readily available for data mining. In such cases, you must identify other sources of data or even plan new data collection initiatives, including surveys. The type of data, its size, and frequency of collection have a direct bearing on the cost of data mining exercise. Therefore, identifying the right kind of data needed for data mining that could answer the questions at reasonable costs is critical.

### Preprocessing Data
Preprocessing data is an important step in data mining. Often raw data are messy, containing erroneous or irrelevant data. In addition, even with relevant data, information is sometimes missing. In the preprocessing stage, you **identify the irrelevant attributes of data and expunge** such attributes from further consideration. At the same time, identifying the erroneous aspects of the data set and flagging them as such is necessary. For instance, human error might lead to inadvertent merging or incorrect parsing of information between columns. Data should be subject to checks to ensure integrity. Lastly, you must **develop a formal method of dealing with missing data** and determine whether the data are missing randomly or systematically.

If the data were missing randomly, a simple set of solutions would suffice. However, when data are missing in a systematic way, you must determine the impact of missing data on the results. For instance, a particular subset of individuals in a large data set may have refused to disclose their income. Findings relying on an individual's income as input would exclude details of those individuals whose income was not reported. This would lead to systematic biases in the analysis. Therefore, you must consider in advance if observations or variables containing missing data be excluded from the entire analysis or parts of it.

### Transforming Data
After the relevant attributes of data have been retained, the next step is to determine the appropriate format in which data must be stored. An important consideration in data mining is to reduce the number of attributes needed to explain the phenomena. This may require transforming data Data reduction algorithms, such as Principal Component Analysis (demonstrated and explained later in the chapter), can reduce the number of attributes without a significant loss in information. In addition, variables may need to be transformed to help explain the phenomenon being studied. For instance, an individual's income may be recorded in the data set as wage income; income from other sources, such as rental properties; support payments from the government, and the like. Aggregating income from all sources will develop a representative indicator for the individual income.

Often you need to transform variables from one type to another. It may be prudent to transform the continuous variable for income into a categorical variable where each record in the database is identified as low, medium, and high-income individual. This could help capture the non-linearities in the underlying behaviors.

### Storing Data
The transformed data must be stored in a format that makes it conducive for data mining. The data must be stored in a format that gives unrestricted and immediate read/write privileges to the data scientist. During data mining, new variables are created, which are written back to the original database, which is why the data storage scheme should facilitate efficiently reading from and writing to the database. It is also important to store data on servers or storage media that keeps the data secure and also prevents the data mining algorithm from unnecessarily searching for pieces of data scattered on different servers or storage media. Data safety and privacy should be a prime concern for storing data.

### Mining Data
After data is appropriately processed, transformed, and stored, it is subject to data mining. This step covers data analysis methods, including parametric and non-parametric methods, and machine-learning algorithms. A good starting point for data mining is data visualization. Multidimensional views of the data using the advanced graphing capabilities of data mining software are very helpful in developing a preliminary understanding of the trends hidden in the data set.

### Evaluating Mining Results
After results have been extracted from data mining, you do a formal evaluation of the results. Formal evaluation could include testing the predictive capabilities of the models on observed data to **see how effective and efficient the algorithms have been in reproducing data.** This is known as an "in-sample forecast". In addition, the **results are shared ith the key stakeholders for feedback**, which is then incorporated in the later iterations of data mining to improve the process.

Data mining and evaluating the results becomes an iterative process such that the analysts use better and improved algorithms to improve the quality of results generated in light of the feedback received from the key stakeholders.

## Lesson Summary

In this lesson, you gained insights into the impact of big data on various aspects of society, from business operations to sports. And developed an understanding of key attributes and challenges associated with big data.
Recap fundamentals of big data and how big data drives digital transformation. How data scientists leverage the essential characteristics of the Cloud to gain insights from big data, the data mining process, and common tools used to process big data.

The availability of vast amounts of data, resulting in what we now call big data, is driving transformation in business and industry and consequently, how we live our daily lives. Organizations realize that we require fundamental changes to their approach to business, impacting every aspect of the organization. The availability of so many disparate amounts of data created by people, tools and machines requires new, innovative, and scalable technology.

Big data drives us to derive real time business insights relating to consumers risk, profit, performance productivity management, and ultimately enhancing business values.

Not everyone agrees on the definition of big data, but people generally agree on the five characteristics of this data value, volume, velocity, variety and veracity. People expect investing time in studying big data will create value. Volume refers to the scale of the data, drivers of volume include increasing collectible data sources and scalable infrastructure. Velocity indicates ever increasing sources of nonstop processes that generate data quickly. Variety reflects that related data comes from different sources, both structured and unstructured. Veracity refers to the quality and origin of data and that it accurately conforms to facts. The development of cloud and cloud technologies enables us to work with big data. Cloud refers to the delivery of on-demand computing resources on a pay-for-use basis.

Cloud computing has five essential characteristics, on-demand, network access, resource pooling, elasticity, and measured service. 
**On-demand** means access to processing, power, storage and network that you need. 
These computing resources can be accessed via a **network** with Internet access.
**Resource pooling** allows providers to service multiple consumers with the resources **dynamically** assigned according to demand, making cloud computing cost efficient.
**Elasticity** means that you can access resources as you need them and automatically scale back when you don't.
With **measured service**, you only pay for what you use or reserve as you go.

You also gain an understanding of how cloud computing addresses challenges related to scalability, collaboration, accessibility and software maintenance, making it a valuable resource for data analysis and other computational tasks.
The Cloud gives you instant access to technologies without needing to install or configure them, and provides updated versions of these tools as they get released. 

Popular open source tools to compute using big data include Apache Hadoop, Apache Hive, and Apache Spark.
**Hadoop** provides distributed storage and processing tools across clusters of computers.
**Hive** is a data warehouse for data query and analysis built on top of Hadoop. Hive allows you to read, write and manage large datasets directly in the Hadoop File system or HDFS or Apache HBase.
**Spark** provides a general purpose data processing engine designed to extract and process large volumes of data for a wide range of applications. 

Big data requires a process called data mining to make use of. This six step process includes goal setting, selecting data sources, preprocessing, transforming, mining, and evaluation.

**goal setting**, you identify key questions you want to answer, concerns about cost and benefits should inform this step. Once you identify the questions, select the data by identifying sources or planning data collection initiatives. 
**preprocessing**, you identify irrelevant attributes of data and enormous aspects of the data by flagging them as necessary.
After preprocessing, you **transform** the data by determining the appropriate format to store the data. 
Now you get to **mine the data**, which includes determining analysis methods and the machine learning algorithms you will use to process the data.
Once the data has been mined, you finally must **evaluate** your outcomes. By testing the predictive capabilities of the models on the observed data to find effectiveness and efficiency of your algorithms. In addition, you share your results with stakeholders.
This entire process should be conducted iteratively as your results from this iteration will inform further data mining efforts.

In summary, big data characteristics that data scientists agree on, even though they might not agree on the exact definition, include value, volume, velocity, and veracity. Data with these qualities is driving transformation across industries and in our daily lives. In large part, cloud technologies enable us to handle big data because they provide ubiquitous access to computational power and storage capacity. Open source cloud tools such as Hadoop, Hive, and Spark leverage these advantages, allowing us to effectively and efficiently mine big data.
