# Glossary

## A

`Analytic Approach`
:	The process of selecting the appropriate method or path to address a specific data science question or problem.

`Analytics`
:	The systematic analysis of data using statistical, mathematical, and computational techniques to uncover insights, patterns, and trends.
	
`Analytics team`
:	A group of professionals, including data scientists and analysts, responsible for performing data analysis and modeling.
	
`Automation`
:	Using tools and techniques to streamline data collection and preparation processes.

## B

`Binary classification model`
:	A model that classifies data into two categories, such as yes/no or stop/go outcomes.
	
`Browser-based application`
:	An application that users access through a web browser, typically on a tablet or other mobile device, to provide easy access to the model's insights.

`Business Understanding`
:	The initial phase of data science methodology involves seeking clarification and understanding the goals, objectives, and requirements of a given task or problem.

## C

`Clustering Association`
:	An approach used to learn about human behavior and identify patterns and associations in data.

`Cohort`
:	A group of individuals who share a common characteristic or experience is studied or analyzed as a unit.

`Cohort study`
:	An observational study where a group of individuals with a specific characteristic or exposure is followed over time to determine the incidence of outcomes or the relationship between exposures and outcomes.

`Congestive Heart Failure (CHF)`
:	A chronic condition in which the heart cannot pump enough blood to meet the body's needs, resulting in fluid buildup and symptoms such as shortness of breath and fatigue.

`CRISP-DM`
:	Cross-Industry Standard Process for Data Mining is a widely used methodology for data mining and analytics projects encompassing six phases: business understanding, data understanding, data preparation, modeling, evaluation, and deployment.
	
`Cyclical methodology`
:	An iterative approach to the data science process, where each stage informs and refines the subsequent stages.

## D

`Data analysis`
:	The process of inspecting, cleaning, transforming, and modeling data to discover useful information, draw conclusions, and support decision-making.

`Data cleansing`
:	The process of identifying and correcting or removing errors, inconsistencies, or inaccuracies in a dataset to improve its quality and reliability
	
`Data Collection`
:	The phase of gathering and assembling data from various sources, including demographic, clinical, coverage, and pharmaceutical information.
	
`Data collection refinement`
:	The process of obtaining additional data elements or information to improve the model's performance.
	
`Data Compilation`
:	The process of organizing and structuring data to create a comprehensive data set.

`Data Formatting`
:	The process of standardizing the data to ensure uniformity and ease of analysis.

`Data integration`
:	The merging of data from multiple sources to remove redundancy and prepare it for further analysis.

`Data Manipulation`
:	The process of transforming data into a usable format.
	
`Data modeling`
:	The stage in the data science methodology where data scientists develop models, either descriptive or predictive, to answer specific questions.

`Data Preparation`
:	The phase where data is cleaned, transformed, and formatted for further analysis, including feature engineering and text analysis.

`Data Preparation`
:	The stage where data is transformed/formatting and organized to facilitate effective analysis and modeling.

`Data Quality`
:	Assessment of data integrity and completeness, addressing missing, invalid, or misleading values.

`Data Quality`
:	Assessment	The evaluation of data integrity, accuracy, and completeness.
	
`Data Set`
:	A collection of data used for analysis and modeling.

`Data science`
:	An interdisciplinary field that combines scientific methods, processes, algorithms, and systems to extract knowledge and insights from structured and unstructured data.

`Data science methodology`
:	A structured approach to solving business problems using data analysis and data-driven insights.
	
`Data science model`
:	The result of data analysis and modeling that provides answers to specific questions or problems.

`Data scientist`
:	A professional using scientific methods, algorithms, and tools to analyze data, extract insights, and develop models or solutions to complex business problems.

`Data scientists`
:	Professionals with data science and analytics expertise who apply their skills to solve business problems.

`Data Requirements`
:	The identification and definition of the necessary data elements, formats, and sources required for analysis.

`Data Understanding`
:	The stage in the data science methodology focused on exploring and analyzing the collected data to ensure that the data is representative of the problem to be solved.

`Data-Driven Insights`
:	Insights derived from analyzing and interpreting data to inform decision-making
	
`DBAs (Database Administrators)`
:	The professionals who are responsible for managing and extracting data from databases.

`Decision tree`
:	A supervised machine learning algorithm that uses a tree-like structure of decisions and their possible consequences to make predictions or classify instances.

`Decision Tree Classification Model`
:	A model that uses a tree-like structure to classify data based on conditions and thresholds provides predicted outcomes and associated probabilities.

`Decision Tree Classifier`
:	A classification model that uses a decision tree to determine outcomes based on specific conditions and thresholds.

`Decision-Tree Model`
:	A model used to review scenarios and identify relationships in data, such as the reasons for patient readmissions
	
`Demographic information`
:	Information about patient characteristics, such as age, gender, and location.

`Descriptive approach`
:	An approach used to show relationships and identify clusters of similar activities based on events and preferences
	
`Descriptive model`
:	A type of model that examines relationships between variables and makes inferences based on observed patterns.

`Descriptive modeling`
:	Modeling technique that focuses on describing and summarizing data, often through statistical analysis and visualization, without making predictions or inferences
	
`Descriptive Statistics`
:	Summary statistics that data scientists use to describe and understand the distribution of variables, such as mean, median, minimum, maximum, and standard deviation. Techniques used to analyze and summarize data, providing initial insights and identifying gaps in data.

`Diagnostic measure based tuning`
:	The process of fine-tuning the model by adjusting parameters based on diagnostic measures and performance indicators.

`Diagnostic measures`
:	The evaluation of a model's performance of a model to ensure that the model functions as intended.

`Discrimination criterion`
:	A measure used to evaluate the performance of the model in classifying different outcomes.

`Domain knowledge`
:	Expertise and understanding of a specific subject area or field, including its concepts, principles, and relevant data

## F

`False-positive rate`
:	The rate at which the model incorrectly identifies negative outcomes as positive.

`Feature`
:	A characteristic or attribute within the data that helps in solving the problem.

`Feature Engineering`
:	The process of creating new features or variables based on domain knowledge to improve machine learning algorithms' performance.

`Feature Extraction`
:	Identifying and selecting relevant features or attributes from the data set.
	
`Feedback`
:	The process of obtaining input and comments from users and stakeholders to refine and improve the data science model.

## G

`Goals and objectives`
:	The sought-after outcomes and specific objectives that support the overall goal of the task or problem.

## H

`Histogram`
:	A graphical representation of the distribution of a dataset, where the data is divided into intervals or bins, and the height of each bar represents the frequency or count of data points falling within that interval.

## I

`Intermediate results`
:	Partial results obtained from predictive modeling can influence decisions on acquiring additional data.

`Iteration`
:	A single cycle or repetition of a process often involves refining or modifying a solution based on feedback or new information.

`Iterative process`
:	A process that involves repeating a series of steps or actions to refine and improve a solution or analysis. Each iteration builds upon the previous one. Iterative and continuous refinement of the methodology based on insights and feedback from data analysis.

## L

`Leaf`
:	The final nodes of a decision tree where data is categorized into specific outcomes.

## M

`Machine Learning`
:	A field of study that enables computers to learn from data without being explicitly programmed, identifying hidden relationships and trends.
	
`Maximum separation`
:	The point where the ROC curve provides the best discrimination between true-positive and false-positive rates, indicating the most effective model.

`Mean`
:	The average value of a set of numbers is calculated by summing all the values and dividing by the total number of values.

`Median`
:	When arranged in ascending or descending order, the middle value in a set of numbers divides the data into two equal halves.
	
`Missing Values`
:	Values that are absent or unknown in the dataset, requiring careful handling during data preparation.

`Model (Conceptual model)`
:	A simplified representation or abstraction of a real-world system or phenomenon used to understand, analyze, or predict its behavior.

`Model building`
:	The process of developing predictive models to gain insights and make informed decisions based on data analysis.
	
`Model Calibration`
:	Adjusting model parameters to improve accuracy and alignment with the initial design.
	
`Model evaluation`
:	The process of assessing the quality and relevance of the model before deployment.
	
`Model refinement`
:	The process of adjusting and improving the data science model based on user feedback and real-world performance.

## O

`Optimal model`
:	The model that provides the maximum separation between the ROC curve and the baseline, indicating higher accuracy and effectiveness.

## P

`Pairwise comparison (correlation)`
:	A statistical technique that measures the strength and direction of the linear relationship between two variables by calculating a correlation coefficient. An analysis to determine the relationships and correlations between different variables.

`Patient cohort`
:	A group of patients with specific criteria selected for analysis in a study or model.

`Pattern`
:	A recurring or noticeable arrangement or sequence in data can provide insights or be used for prediction or classification.

`Predictive model`
:	A model used to determine probabilities of an action or outcome based on historical data.
	
`Predictive modeling`
:	The building of models to predict future outcomes based on historical data.

`Predictors`
:	Variables or features in a model that are used to predict or explain the outcome variable or target variable.

`Prioritization`
:	The process of organizing objectives and tasks based on their importance and impact on the overall goal.

`Problem solving`
:	The process of addressing challenges and finding solutions to achieve desired outcomes.

## R

`Receiver Operating Characteristic (ROC)`
:	Originally developed for military radar, the military used this statistical curve to assess the performance of binary classification models.
	
`Redeployment`
:	The process of implementing a refined model and intervention actions after incorporating feedback and improvements.

`Relative misclassification cost`
:	This measurement is a parameter in model building used to tune the trade-off between true-positive and false-positive rates.
	
`Review process`
:	The systematic assessment and evaluation of the data science model's performance and impact.

`ROC curve (Receiver Operating Characteristic curve)`
:	A diagnostic tool used to determine the optimal classification model's performance.

## S

`Separation`
:	Separation is the degree of discrimination achieved by the model in correctly classifying outcomes.

`Solution deployment`
:	The process of implementing and integrating the data science model into the business or organizational workflow.

`Solution owner`
:	The individual or team responsible for overseeing the deployment and management of the data science solution.

`Stakeholders`
:	Individuals or groups with a vested interest in the data science model's outcome and its practical application, such as solution owners, marketing, application developers, and IT administration.

`Standard deviation`
:	A measure of the dispersion or variability of a set of values from their mean; It provides information about the spread or distribution of the data.

`Statistical analysis`
:	Stand deviations are applied to problems that require counts, such as yes/no answers or classification tasks.

`Statistical significance testing`
:	Evaluation technique to verify that data is appropriately handled and interpreted within the model.

`Statistics`
:	The collection, analysis, interpretation, presentation, and organization of data to understand patterns, relationships, and variability in the data.
	
`Storytelling`
:	Storytelling is the art of conveying your message, or ideas through a narrative structure that engages, entertains, and resonates with the audience.

`Structured data (data model)`
:	Data organized and formatted according to a predefined schema or model and is typically stored in databases or spreadsheets.

## T

`Test environment`
:	A controlled setting where the data science model is evaluated and refined before full-scale implementation.

`Text analysis data mining`
:	The process of extracting useful information or knowledge from unstructured textual data through techniques such as natural language processing, text mining, and sentiment analysis. Steps to analyze and manipulate textual data, extracting meaningful information and patterns.

`Text Analysis Groupings`
:	Creating meaningful groupings and categories from textual data for analysis.

`Threshold value`
:	The specific value used to split data into groups or categories in a decision tree.

`Training set`
:	A subset of data used to train or fit a machine learning model; consists of input data and corresponding known or labeled output values.
	
`True-positive rate`
:	The rate at which the model correctly identifies positive outcomes.

## U

`Unavailable data`
:	Data elements are not currently accessible or integrated into the data sources.

`Univariate`
:	Modeling analysis focused on a single variable or feature at a time, considering its characteristics and relationship to other variables independently.

`Unstructured data`
:	Data that does not have a predefined structure or format, typically text images, audio, or video, requires special techniques to extract meaning or insights.

## V

`Visualization`
:	The process of representing data visually to gain insights into its content and quality.

`Visualization techniques`
:	Methods and tools that data scientists use to create visual representations or graphics that enhance the accessibility and understanding of data patterns, relationships, and insights.
